{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict FLSA code with job descriptions and compensation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import os\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Input, Lambda, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "## Code will work with either tensorflow version, but needs to be executed eagerly if 1.X\n",
    "if tf.__version__[0] == '1':\n",
    "    tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_strat(type='cpu', tpu=None, zone=None, project=None):\n",
    "    if type == 'cpu':\n",
    "        return tf.distribute.OneDeviceStrategy(device='/cpu:0')\n",
    "    elif type == 'gpu':\n",
    "        return tf.distribute.OneDeviceStrategy(device='/gpu:0')\n",
    "    elif type == 'tpu':\n",
    "        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "            tpu=tpu, zone=zone, project=project)\n",
    "        tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "        tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
    "        return tpu_strategy\n",
    "    elif type == 'mirror':\n",
    "        return tf.distribute.MirroredStrategy(devices=['/gpu:0', '/gpu:1'])\n",
    "    else:\n",
    "        raise ValueError('Available strategy types are cpu, gpu, tpu, and mirror')\n",
    "\n",
    "def build_model(strategy, lr, activation, n_layers=3, layer_units=None):\n",
    "    if layer_units is None:\n",
    "        layer_units = [100] * n_layers\n",
    "    with strategy.scope():\n",
    "        with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "            bc = bert.loader.StockBertConfig.from_json_string(reader.read())\n",
    "            bert_params = bert.loader.map_stock_config_to_params(bc)\n",
    "            bert_layer = BertModelLayer().from_params(bert_params, name='bert')\n",
    "\n",
    "        bert_in = Input(shape=(max_seq_len,), dtype='int32', name=\"bert_input\")\n",
    "        bert_out = bert_layer(bert_in)\n",
    "        bert_out = Lambda(lambda seq: seq[:, 0, :])(bert_out)\n",
    "        bert_out = Dropout(0.5)(bert_out)\n",
    "        x = Dense(768, activation=None, kernel_initializer='he_normal')(bert_out)\n",
    "        x = BatchNormalization(momentum=0.9)(x)\n",
    "        x = Activation(activ)(x)\n",
    "        out = Dense(units=n_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=bert_in, outputs=out)\n",
    "        bert.loader.load_stock_weights(bert_layer, bert_ckpt_file)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=keras.optimizers.Adam(lr),\n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Define Environment Variables and Hyperparameters</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment Variables\n",
    "distr_strat = 'gpu'\n",
    "do_train = True\n",
    "do_eval = True\n",
    "\n",
    "## BERT Params\n",
    "model_id = 1\n",
    "from_scratch = True\n",
    "bert_type = 'uncased_base'\n",
    "do_lower_case = True\n",
    "max_seq_len = 256\n",
    "activ = 'elu'\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up files, file names, and directories to be referenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include strategy variables if tpu\n",
    "if distr_strat == 'tpu':\n",
    "    tpu_name = 'node-2'\n",
    "    tpu_zone = 'us-central1-c'\n",
    "    tpu_proj = 'cedar-pottery-252818'\n",
    "else:\n",
    "    tpu_name = None\n",
    "    tpu_zone = None\n",
    "    tpu_proj = None\n",
    "\n",
    "## Relevant directories - saved model metadata in model_comparison.xlsx by id\n",
    "proj_dir = 'gs://eri-ml-bucket-1/flsa_prediction'\n",
    "data_dir = os.path.join(proj_dir, 'data', 'bert_only')\n",
    "tf.io.gfile.makedirs('saved_models')\n",
    "model_file = os.path.join('saved_models', 'model.{:02d}.h5'.format(model_id))\n",
    "if not from_scratch and not tf.io.gfile.exists(model_file):\n",
    "    tf.io.gfile.copy(os.path.join(proj_dir, model_file), model_file)\n",
    "\n",
    "## BERT pretrained files\n",
    "bert_dir = os.path.join(os.getcwd(), 'data', bert_type)\n",
    "bert_ckpt_file = os.path.join(bert_dir, 'bert_model.ckpt')\n",
    "bert_config_file = os.path.join(bert_dir, 'bert_config.json')\n",
    "bert_vocab_file = os.path.join(bert_dir, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw data from GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    file = os.path.join(data_dir, 'X_train.npy')\n",
    "    with tf.io.gfile.GFile(file, 'rb') as f:\n",
    "        X_train = np.load(f, allow_pickle=True)\n",
    "    file = os.path.join(data_dir, 'y_train.npy')\n",
    "    with tf.io.gfile.GFile(file, 'rb') as f:\n",
    "        y_train = np.load(f, allow_pickle=True)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_ds = train_ds.shuffle(buffer_size=100).batch(batch_size)\n",
    "    train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    train_size = len(y_train)\n",
    "    del X_train, y_train\n",
    "    \n",
    "    file = os.path.join(data_dir, 'X_valid.npy')\n",
    "    with tf.io.gfile.GFile(file, 'rb') as f:\n",
    "        X_valid = np.load(f, allow_pickle=True)\n",
    "    file = os.path.join(data_dir, 'y_valid.npy')\n",
    "    with tf.io.gfile.GFile(file, 'rb') as f:\n",
    "        y_valid = np.load(f, allow_pickle=True)\n",
    "    valid_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "    valid_ds = valid_ds.shuffle(buffer_size=100).batch(batch_size)\n",
    "    valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    valid_size = len(y_valid)\n",
    "    del X_valid, y_valid\n",
    "    \n",
    "if do_eval:\n",
    "    file = os.path.join(data_dir, 'X_test.npy')\n",
    "    with tf.io.gfile.GFile(file, 'rb') as f:\n",
    "        X_test = np.load(f, allow_pickle=True)\n",
    "    file = os.path.join(data_dir, 'y_test.npy')\n",
    "    with tf.io.gfile.GFile(file, 'rb') as f:\n",
    "        y_test = np.load(f, allow_pickle=True)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    test_ds = test_ds.shuffle(buffer_size=100).batch(batch_size)\n",
    "    test_ds = test_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    test_size = len(y_test)\n",
    "    del X_test, y_test\n",
    "\n",
    "n_classes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Fine-Tuning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build pure BERT model from \"scratch\" or load previous h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = build_strat(type=distr_strat, tpu=tpu_name, zone=tpu_zone, project=tpu_proj)\n",
    "if from_scratch:\n",
    "    model = build_model(strat, lr, activ, n_layers=n_layers, layer_units=layer_units)\n",
    "else:\n",
    "    model = keras.models.load_model(model_file, custom_objects={'BertModelLayer': bert.BertModelLayer,\n",
    "                                                                'DenseBlock': DenseBlock})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, saving the epoch with the highest accuracy locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds, \n",
    "    epochs=bert_epochs, \n",
    "    validation_data=valid_ds)\n",
    "\n",
    "trained_bert_file = os.path.join('saved_models', 'trained_bert_model.{:02d}.h5'.format(model_id))\n",
    "model.save(trained_bert_file)\n",
    "tf.io.gfile.copy(trained_bert_file, os.path.join(proj_dir, trained_bert_file), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].trainable = False\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=7,\n",
    "    restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=model_file,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)\n",
    "his = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=epochs, \n",
    "    initial_epoch=bert_epochs, \n",
    "    callbacks=[early_stop, checkpoint])\n",
    "\n",
    "his_file = os.path.join(proj_dir, 'saved_models', 'fit_history.csv')\n",
    "his = pd.DataFrame(his.history)\n",
    "his.to_csv(his_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.gfile.copy(model_file, os.path.join(proj_dir, model_file), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>Update GCP with the latest version of this script</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.gfile.copy('model.ipynb', os.path.join(proj_dir, 'model.ipynb'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve_model = os.path.join('saved_models', 'model_{%02d}'.format(model_id))\n",
    "model.save(serve_model, save_format='tf')\n",
    "!gsutil -m cp -R $serve_model gs://eri-ml-bucket-1/ml_job_match/$serve_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
